# Genomic Mechanistic Interpretability

Mechanistic interpretability analysis of genomic foundation models using attention visualization, activation patching, circuit discovery, and sparse autoencoders. This project analyzes **DNA-BERT-6** on **sQTL** (splicing QTL) and **eQTL** (expression QTL) variants to understand how the model processes functional genomic variants.

## Features

- **Model**: DNA-BERT-6 (6-layer BERT model trained on genomic sequences)
- **Primary Datasets**: sQTL and eQTL variants from GTEx
- **3 Analysis Methods**:
  - Attention visualization and pattern analysis
  - Activation patching for causal analysis
  - Sparse autoencoder for feature learning

## Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/ranaabarghout/genomic-mechanistic-interpretability.git
cd genomic-mechanistic-interpretability

# Install dependencies
pip install -r requirements.txt

# Setup genomic-FM data loaders (required)
cd ../
git clone https://github.com/ranaabarghout/genomic-FM.git
```

### Basic Usage

**Run complete analysis on sQTL data (1000 samples):**
```bash
python scripts/run_multi_dataset_analysis.py --dataset sqtl --num-samples 1000 --mechanistic-attention --train-sae --sae-epochs 50
```

**Run complete analysis on eQTL data (1000 samples):**
```bash
python scripts/run_multi_dataset_analysis.py --dataset eqtl --num-samples 1000 --mechanistic-attention --train-sae --sae-epochs 50
```

**Quick test mode (50 samples, ~3 min):**
```bash
python scripts/run_multi_dataset_analysis.py --dataset eqtl --quick-test --train-sae
```


## Project Structure

```
genomic-mechanistic-interpretability/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/              # Data loaders for each dataset
â”‚   â”œâ”€â”€ interpretability/  # Analysis modules (attention, patching, SAE, circuit)
â”‚   â””â”€â”€ models/            # Model loading utilities
â”œâ”€â”€ scripts/               # Executable analysis scripts
â”‚   â”œâ”€â”€ run_multi_dataset_analysis.py    # Main unified analysis pipeline
â”‚   â”œâ”€â”€ run_hidden_state_analysis.py     # Hidden state analysis only
â”‚   â”œâ”€â”€ run_mechanistic_attention_analysis.py  # Attention analysis only
â”‚   â”œâ”€â”€ run_activation_patching.py       # Activation patching only
â”‚   â”œâ”€â”€ run_circuit_analysis.py          # Circuit discovery only
â”‚   â”œâ”€â”€ run_sparse_autoencoder.py        # SAE training only
â”‚   â””â”€â”€ archive/                          # Legacy scripts
â”œâ”€â”€ outputs/               # Analysis results and visualizations
â”‚   â”œâ”€â”€ eqtl_analysis/    # eQTL analysis outputs
â”‚   â””â”€â”€ sqtl_analysis/    # sQTL analysis outputs
â”œâ”€â”€ report/                # LaTeX report template and figures
â”œâ”€â”€ docs/                  # Documentation
â””â”€â”€ root/data/             # Data storage directory
```

## Supported Datasets

| Dataset | Description | Status |
|---------|-------------|--------|
| **sQTL** | Splicing QTL variants from GTEx | âœ… Available |
| **eQTL** | Expression QTL variants from GTEx | âœ… Available |
| **ClinVar** | Pathogenic/benign variants | ðŸ”œ Future support |
| **GWAS** | Trait-associated variants | ðŸ”œ Future support |
| **MAVE** | Experimental variant effects | ðŸ”œ Future support |

Currently, the analysis pipeline is optimized for **sQTL** and **eQTL** variants. Support for additional datasets (ClinVar, GWAS, MAVE) will be added in future releases.

## Analysis Methods

### 1. Attention Visualization
Analyzes attention patterns to understand which sequence positions the model focuses on for variant classification.

### 2. Activation Patching
Causal intervention analysis to identify which model components are critical for predictions.

### 3. Sparse Autoencoder
Learns interpretable features from model activations to understand internal representations.

### 4. Circuit Discovery (Work in Progress)
Will discover functional circuits of attention heads that work together for specific tasks. This analysis method will be available in future releases.

## Output

Each analysis generates:
- **Visualizations**: PNG plots of attention, circuits, features
- **Reports**: Text summaries with key findings
- **Models**: Trained SAE models (if `--train-sae`)
- **Summary**: Overall analysis report

Results are saved to `outputs/<dataset>_analysis/analysis_<timestamp>/`

## Common Options

```bash
--dataset            Dataset to analyze (sqtl, eqtl, clinvar, gwas, mave)
--num-samples        Number of samples to analyze (default: 200)
--mechanistic-attention  Use enhanced mechanistic attention analysis
--train-sae          Train sparse autoencoder (adds ~2 min)
--sae-epochs         Number of SAE training epochs (default: 50)
--run-circuit        Run circuit analysis (NOT CURRENTLY RECOMMENDED, optional)
--quick-test         Fast test with 50 samples (~3 min)
--output-dir         Custom output directory
```

## Requirements

- Python 3.8+
- PyTorch 2.0+
- Transformers (Hugging Face)
- DNA-BERT-6 model (zhihan1996/DNA_bert_6, auto-downloaded)
- genomic-FM repository (for data loaders)

See `requirements.txt` for full dependencies. Please also see the genomic-FM dependency instructions for additional packages that might be needed!

## Citation

If you use this code, please cite:

```bibtex
@software{barghout2026genomic,
  title={Genomic Mechanistic Interpretability},
  author={Barghout, Rana A.},
  year={2026},
  url={https://github.com/ranaabarghout/genomic-mechanistic-interpretability}
}
```

## License

MIT License - see LICENSE file for details.

